{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "from ruamel.yaml import YAML\n",
    "\n",
    "# === Variables ===\n",
    "# Path to YAML config file\n",
    "config_file_path = r\"G:\\My Drive\\Wantrepreneurialism\\Active\\spend-analytics\\Tesco Clubcards\\3) Code\\config.yaml\"\n",
    "\n",
    "# YAML handler\n",
    "yaml_handler = YAML()\n",
    "yaml_handler.preserve_quotes = True\n",
    "yaml_handler.width = 4096  # prevent wrapping of long strings\n",
    "\n",
    "# === Functions ===\n",
    "\n",
    "# Function to load config from YAML\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        return yaml_handler.load(file)\n",
    "\n",
    "# Function to load last unzip timestamp\n",
    "def load_last_unzip_time(config_data):\n",
    "    return datetime.fromisoformat(config_data['variables']['opener_variables']['last_unzip'])\n",
    "\n",
    "# Function to update the unzip timestamp in config\n",
    "def update_last_unzip_time(config_path, config_data, updated_timestamp):\n",
    "    config_data['variables']['opener_variables']['last_unzip'] = updated_timestamp.isoformat()\n",
    "    with open(config_path, 'w') as file:\n",
    "        yaml_handler.dump(config_data, file)\n",
    "\n",
    "# Function to unzip new or modified files\n",
    "def unzip_new_or_updated_files(zip_folder, unzipped_output_root_folder, last_unzip_timestamp):\n",
    "    newly_unzipped_list = []\n",
    "    latest_timestamp = last_unzip_timestamp\n",
    "    for file_name in os.listdir(zip_folder):\n",
    "        if file_name.lower().endswith('.zip'):\n",
    "            zip_file_path = os.path.join(zip_folder, file_name)\n",
    "            file_modified_time = datetime.fromtimestamp(os.path.getmtime(zip_file_path))\n",
    "            if file_modified_time > last_unzip_timestamp:\n",
    "                destination_folder = os.path.join(unzipped_output_root_folder, os.path.splitext(file_name)[0])\n",
    "                newly_unzipped_list.append(destination_folder)\n",
    "                os.makedirs(destination_folder, exist_ok=True)\n",
    "                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(destination_folder)\n",
    "                if file_modified_time > latest_timestamp:\n",
    "                    latest_timestamp = file_modified_time\n",
    "    return latest_timestamp, newly_unzipped_list\n",
    "\n",
    "def grab_UIDs(df_fact_transactions, df_all):\n",
    "    # Create a lower-case, stripped join key for case-insensitive matching\n",
    "    df_fact_transactions['key'] = df_fact_transactions['name'].str.lower().str.strip()\n",
    "    df_all['key'] = df_all['product name'].str.lower().str.strip()\n",
    "\n",
    "    # Perform a left join to assign UIDs to each fact transaction\n",
    "    df_join = pd.merge(df_fact_transactions, df_all[['UID', 'key']], on='key', how='left')\n",
    "\n",
    "    # Identify the keys that did not get a UID (i.e. missing matches)\n",
    "    missing_keys = df_join.loc[df_join['UID'].isna(), 'key'].unique()\n",
    "\n",
    "    if missing_keys.size:\n",
    "        # For missing keys, get the original product name (preserving capitalization)\n",
    "        new_products = df_fact_transactions[df_fact_transactions['key'].isin(missing_keys)] \\\n",
    "                    .drop_duplicates('key')[['name', 'key']]\n",
    "        \n",
    "        # Determine the current maximum UID number in df_all; default to 0 if empty\n",
    "        max_id = df_all['UID'].str.extract(r'ID_(\\d+)')[0].astype(int).max() if not df_all.empty else 0\n",
    "        \n",
    "        # Generate new UIDs for the missing keys\n",
    "        new_products['UID'] = ['ID_' + str(i) for i in range(max_id + 1, max_id + 1 + len(new_products))]\n",
    "        \n",
    "        # Rename 'name' to 'product name' to match df_all structure\n",
    "        new_products = new_products.rename(columns={'name': 'product name'})\n",
    "        \n",
    "        # Append the new product rows (with UID and key) to df_all\n",
    "        df_all = pd.concat([df_all, new_products[['UID', 'product name', 'key']]], ignore_index=True)\n",
    "\n",
    "    # Re-join so every fact transaction now has a valid UID\n",
    "    df_fact_transactions = pd.merge(df_fact_transactions, df_all[['UID', 'key']], on='key', how='left').drop(columns='key')\n",
    "    df_all = df_all.drop(columns='key')\n",
    "\n",
    "    return df_fact_transactions, df_all\n",
    "\n",
    "# === Execution ===\n",
    "config_data = load_config(config_file_path)  # load full config\n",
    "last_unzip_timestamp = load_last_unzip_time(config_data)  # load last unzip time\n",
    "\n",
    "zip_folder = os.path.join(config_data['file_paths']['opener_paths']['input_root_folder'], \"1) ZIPs\")  # ZIP folder path\n",
    "unzipped_output_root_folder = config_data['file_paths']['opener_paths']['unzipped_output_folder']  # unzipped destination folder\n",
    "final_output_root_folder = config_data['file_paths']['opener_paths']['opened_output_folder']  # data destination folder\n",
    "all_items_file = config_data['file_paths']['main_paths']['all_item_input_file']  # master item list\n",
    "\n",
    "df_all = pd.read_excel(all_items_file)\n",
    "\n",
    "latest_unzip_timestamp, newly_unzipped_list = unzip_new_or_updated_files(zip_folder, unzipped_output_root_folder, last_unzip_timestamp)\n",
    "update_last_unzip_time(config_file_path, config_data, latest_unzip_timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(newly_unzipped_list):\n",
    "    for unzipped_folder in newly_unzipped_list:\n",
    "\n",
    "        # Grab the json inside this folder\n",
    "        unzipped_file = next(f for f in os.listdir(unzipped_folder) if f.endswith('.json'))\n",
    "\n",
    "        # Load the JSON file\n",
    "        with open(os.path.join(unzipped_folder, unzipped_file), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Top-level identifiers\n",
    "        customerId = data.get('customerId')\n",
    "        requestId = data.get('requestId')\n",
    "\n",
    "        # Lists to hold our records\n",
    "        fact_transactions = []\n",
    "        dim_basket = []\n",
    "\n",
    "        # Process each purchase in the 'purchases' list\n",
    "        for purchase in data.get('purchases', []):\n",
    "            # Extract basket-level (DIM Basket) information\n",
    "            timestamp = purchase.get('timestamp')\n",
    "            purchase_type = purchase.get('type')\n",
    "            says = purchase.get('says')\n",
    "            basketValueGross = purchase.get('basketValueGross')\n",
    "            overallBasketSavings = purchase.get('overallBasketSavings')\n",
    "            basketValueNet = purchase.get('basketValueNet')\n",
    "            numberOfItems = purchase.get('numberOfItems')\n",
    "            \n",
    "            # Assume first element of payment list holds our payment info\n",
    "            payment = purchase.get('payment', [])\n",
    "            if payment:\n",
    "                payment_record = payment[0]\n",
    "                payment_type = payment_record.get('type')\n",
    "                payment_category = payment_record.get('category')\n",
    "                payment_amount = payment_record.get('amount')\n",
    "            else:\n",
    "                payment_type = payment_category = payment_amount = None\n",
    "\n",
    "            # Create a DIM Basket record\n",
    "            basket_record = {\n",
    "                'timestamp': timestamp,\n",
    "                'type': purchase_type,\n",
    "                'says': says,\n",
    "                'basketValueGross': basketValueGross,\n",
    "                'overallBasketSavings': overallBasketSavings,\n",
    "                'basketValueNet': basketValueNet,\n",
    "                'numberOfItems': numberOfItems,\n",
    "                'payment_type': payment_type,\n",
    "                'payment_category': payment_category,\n",
    "                'payment_amount': payment_amount,\n",
    "                'customerId': customerId,\n",
    "                'requestId': requestId,\n",
    "            }\n",
    "            dim_basket.append(basket_record)\n",
    "            \n",
    "            # Process each item in the 'items' list for Fact Transactions\n",
    "            for item in purchase.get('items', []):\n",
    "                fact_record = {\n",
    "                    'name': item.get('name'),\n",
    "                    'quantity': item.get('quantity'),\n",
    "                    'price': item.get('price'),\n",
    "                    'volume': item.get('volume'),\n",
    "                    'timestamp': timestamp  # from basket level\n",
    "                }\n",
    "                fact_transactions.append(fact_record)\n",
    "\n",
    "        # Convert lists to DataFrames\n",
    "        df_fact_transactions = pd.DataFrame(fact_transactions)\n",
    "        df_dim_basket = pd.DataFrame(dim_basket)\n",
    "\n",
    "        # Construct final data output path\n",
    "        data_output_path = os.path.join(final_output_root_folder, unzipped_file)\n",
    "        data_output_path = data_output_path.replace(\".json\", \"\")\n",
    "\n",
    "        # Collect UIDs or extend master list if new products\n",
    "        df_fact_transactions, df_all = grab_UIDs(df_fact_transactions, df_all)\n",
    "\n",
    "        # Save to CSV\n",
    "        os.makedirs(data_output_path, exist_ok=True)\n",
    "        df_fact_transactions.to_csv(os.path.join(data_output_path, \"FACT_transactions.csv\"), index=False)\n",
    "        df_dim_basket.to_csv(os.path.join(data_output_path, \"DIM_basket.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
