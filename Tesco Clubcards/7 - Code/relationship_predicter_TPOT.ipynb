{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Specify the folder containing the Excel files\n",
    "folder_path = './4 - Processed Data Files'\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Filter only Excel files\n",
    "excel_files = [file for file in file_list if file.endswith('.xlsx')]\n",
    "\n",
    "# Sort the file list to ensure consistent ordering\n",
    "excel_files.sort()\n",
    "\n",
    "# Iterate through the Excel files and create DataFrames\n",
    "dataframes = {}\n",
    "for i, file in enumerate(excel_files, start=1):\n",
    "    df_name = f'df{i}'\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    dataframes[df_name] = pd.read_excel(file_path)\n",
    "\n",
    "merged_df = pd.concat(dataframes.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assigning DataFrames to variables\n",
    "# df1 = dataframes['df1']\n",
    "# df2 = dataframes['df2']\n",
    "# df3 = dataframes['df3']\n",
    "# df4 = dataframes['df4']\n",
    "# df5 = dataframes['df5']\n",
    "# df6 = dataframes['df6']\n",
    "# df7 = dataframes['df7']\n",
    "# df8 = dataframes['df8']\n",
    "# df9 = dataframes['df9']\n",
    "# df10 = dataframes['df10']\n",
    "# df11 = dataframes['df11']\n",
    "# df12 = dataframes['df12']\n",
    "# df13 = dataframes['df13']\n",
    "# df14 = dataframes['df14']\n",
    "# df15 = dataframes['df15']\n",
    "\n",
    "# # Concatenate DataFrames vertically\n",
    "# merged_df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop columns where all values are \"NA\"\n",
    "merged_df = merged_df.replace(\"NA\", np.nan)  # Convert \"NA\" strings to actual NaN values\n",
    "merged_df = merged_df.dropna(axis=1, how='all')  # Drop columns with all NaN values\n",
    "\n",
    "# For remaining 'NA' values not in fully 'NA' columns, replace with 0\n",
    "merged_df = merged_df.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the corrected dictionary to a DataFrame\n",
    "mock_df_corrected = merged_df\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "mock_df_corrected['timeStamp'] = pd.to_datetime(mock_df_corrected['timeStamp'])\n",
    "\n",
    "# Extract time-based features\n",
    "mock_df_corrected['minute'] = mock_df_corrected['timeStamp'].dt.minute\n",
    "mock_df_corrected['hour'] = mock_df_corrected['timeStamp'].dt.hour\n",
    "mock_df_corrected['weekday'] = mock_df_corrected['timeStamp'].dt.weekday  # Monday=0, Sunday=6\n",
    "mock_df_corrected['day_of_month'] = mock_df_corrected['timeStamp'].dt.day\n",
    "\n",
    "# Now, you can drop the 'timeStamp' column to avoid dtype issues in TPOT\n",
    "mock_df_corrected.drop(['timeStamp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['benedict.s@hotmail.co.uk', 'ellenmcarthur97@gmail.com',\n",
       "       'oms42deadman@gmail.com', 'shutko.vladyslav@gmail.com'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"customer_email\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "friendship_info = {\n",
    "   \"benedict.s@hotmai.co.uk\": [\"ellenmcarthur97@gmail.com'\", \"oms42deadman@gmail.com\", \"shutko.vladyslav@gmail.com\"],\n",
    "   \"ellenmcarthur97@gmail.com\": [\"benedict.s@hotmail.co.uk\"],\n",
    "   \"oms42deadman@gmail.com\": [\"benedict.s@hotmail.co.uk\", \"shutko.vladyslav@gmail.com\"],\n",
    "   \"shutko.vladyslav@gmail.com\": [\"benedict.s@hotmail.co.uk\", \"oms42deadman@gmail.com\"],   \n",
    "}\n",
    "\n",
    "# Extract unique email addresses (individuals) from the mock dataset\n",
    "individuals = mock_df_corrected['customer_email'].unique()\n",
    "\n",
    "# Initialize a DataFrame with zeros, using individuals as both rows and columns\n",
    "friendship_matrix = pd.DataFrame(0, index=individuals, columns=individuals)\n",
    "\n",
    "# Populate the matrix based on the friendship_info dictionary\n",
    "for person, friends in friendship_info.items():\n",
    "    for friend in friends:\n",
    "        friendship_matrix.loc[person, friend] = 1\n",
    "        friendship_matrix.loc[friend, person] = 1  # Friendship is mutual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming mock_df_corrected is our transaction dataset\n",
    "features = mock_df_corrected.drop(['customer_email'], axis=1)  # Excluding email for feature encoding\n",
    "features_encoded = pd.get_dummies(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bened\\AppData\\Local\\Temp\\ipykernel_8780\\4208845645.py:7: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  individual_features = features_with_email.groupby('customer_email').mean()\n"
     ]
    }
   ],
   "source": [
    "# Assuming `features_encoded` is your encoded transaction data\n",
    "# Let's first ensure the customer email is included for aggregation\n",
    "mock_df_corrected['customer_email'] = mock_df_corrected['customer_email'].astype('category')\n",
    "features_with_email = pd.concat([mock_df_corrected['customer_email'], features_encoded], axis=1)\n",
    "\n",
    "# Aggregate features by customer email\n",
    "individual_features = features_with_email.groupby('customer_email').mean()\n",
    "\n",
    "# Ensure the index is consistent for later operations\n",
    "individual_features = individual_features.reindex(friendship_matrix.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for i, email_i in enumerate(friendship_matrix.index):\n",
    "    for j, email_j in enumerate(friendship_matrix.columns):\n",
    "        if i < j:  # Avoid duplicate pairs and self-pairing\n",
    "            # Combine features of both individuals in the pair\n",
    "            features_pair = pd.concat([individual_features.loc[email_i], individual_features.loc[email_j]], axis=0).to_list()\n",
    "            X_train_list.append(features_pair)\n",
    "            # Corresponding friendship status\n",
    "            y_train_list.append(friendship_matrix.loc[email_i, email_j])\n",
    "\n",
    "# Convert lists to DataFrame and Series for X_train and y_train\n",
    "X_train = pd.DataFrame(X_train_list)\n",
    "y_train = pd.Series(y_train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      "Generation 1 - Current best internal CV score: 0.8803030303030303\n",
      "                                                                                 \n",
      "Generation 2 - Current best internal CV score: 0.8803030303030303\n",
      "                                                                               \n",
      "Generation 3 - Current best internal CV score: 0.8803030303030303\n",
      "                                                                               \n",
      "Generation 4 - Current best internal CV score: 0.8803030303030303\n",
      "                                                                               \n",
      "Generation 5 - Current best internal CV score: 0.8803030303030303\n",
      "                                                                               \n",
      "Generation 6 - Current best internal CV score: 0.8803030303030303\n",
      "                                                                               \n",
      "Generation 7 - Current best internal CV score: 0.8803030303030303\n",
      "                                                                               \n",
      "Generation 8 - Current best internal CV score: 0.8803030303030303\n",
      "                                                                                \n",
      "Generation 9 - Current best internal CV score: 0.8803030303030303\n",
      "                                                                                \n",
      "Generation 10 - Current best internal CV score: 0.8803030303030303\n",
      "                                                                                \n",
      "Best pipeline: XGBClassifier(input_matrix, learning_rate=0.001, max_depth=9, min_child_weight=7, n_estimators=100, n_jobs=1, subsample=0.45, verbosity=0)\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "import sklearn.model_selection\n",
    "\n",
    "# Assuming your X_train and y_train are correctly prepared\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_train, y_train, \n",
    "                                                                            test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize TPOT classifier with a mix of specified and default parameters\n",
    "tpot = TPOTClassifier(\n",
    "    generations=10,  # Custom: 10 generations for the optimization process.\n",
    "    population_size=100,  # Custom: 100 individuals in the population per generation.\n",
    "    verbosity=2,  # Custom: Show more detailed progress and pipeline information.\n",
    "    random_state=42,  # Custom: Ensure reproducibility of your results.\n",
    "    scoring=None,  # Default: Use TPOT's default scoring method (accuracy for classification).\n",
    "    cv=None,  # Default: TPOT will use a 5-fold cross-validation.\n",
    "    max_time_mins=None,  # Default: No maximum time limit on the optimization process.\n",
    "    early_stop=None,  # Default: No early stopping. Runs for the full number of generations.\n",
    "    config_dict=None  # Default: TPOT will use its default configuration dictionary.\n",
    ")\n",
    "\n",
    "# Fit the TPOT model to your data\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(f\\nModel accruacy score: {tpot.score(X_test, y_test)}%\")\n",
    "\n",
    "# Optionally, export the pipeline to a Python file\n",
    "# tpot.export('tpot_best_pipeline.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
